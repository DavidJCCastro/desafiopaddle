{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca5e1da-7081-402a-93eb-f183d7439e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf10b8c-ed9e-4d03-a067-fb440a50568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/gigag/Documents/job/desafiopaddle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67c1e2-ba50-40ba-8251-e36ce42e9b6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Undistorting vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05c852-f1b5-4769-84f2-f4928a6a27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_video(input_path, output_path, calibration_images=None):\n",
    "    \"\"\"\n",
    "    Remove lens distortion from a video file\n",
    "    \n",
    "    Parameters:\n",
    "    - input_path: Path to the input video file\n",
    "    - output_path: Path to save the undistorted video\n",
    "    - calibration_images: Optional list of paths to chessboard images for calibration\n",
    "                         If None, uses default estimation values\n",
    "    \"\"\"\n",
    "    # Use estimated values for a typical wide-angle camera\n",
    "    # These are default values, adjust based on your specific camera if known\n",
    "    print(\"Using default camera parameters (no calibration images provided)\")\n",
    "    # Camera matrix (focal length and optical centers)\n",
    "    mtx = np.array([\n",
    "        [1000, 0, 960],  # fx, 0, cx\n",
    "        [0, 1000, 540],  # 0, fy, cy\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    # Distortion coefficients [k1, k2, p1, p2, k3]\n",
    "    dist = np.array([[-0.3, 0.1, 0, 0, -0.02]])\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate optimal camera matrix\n",
    "    newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (width, height), 1, (width, height))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Process each frame\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Undistort the frame\n",
    "            dst = cv2.undistort(frame, mtx, dist, None, newcameramtx)\n",
    "            \n",
    "            # Crop the image (optional)\n",
    "            x, y, w, h = roi\n",
    "            dst = dst[y:y+h, x:x+w]\n",
    "            \n",
    "            # Resize back to original dimensions if needed\n",
    "            if dst.shape[1] != width or dst.shape[0] != height:\n",
    "                dst = cv2.resize(dst, (width, height))\n",
    "            \n",
    "            # Write the undistorted frame\n",
    "            out.write(dst)\n",
    "            \n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Undistorted video saved to {output_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786c87d-23b5-4ffe-8fe2-9771c1e7be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "undistort_video(\"10secs.mp4\", \"undist.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cdaed-8cb2-475b-ac70-f37acc7367ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rotating to straighten the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ccb090-e4d7-4140-8671-c84404173956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_court_lines_for_alignment(frame):\n",
    "    \"\"\"\n",
    "    Detect main court lines just for alignment purposes\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "    \n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    \n",
    "    # Dilate to connect edge components\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Save intermediate results for debugging\n",
    "    cv2.imwrite(\"edges_detected.jpg\", edges)\n",
    "    \n",
    "    # Detect lines using Hough Line Transform\n",
    "    lines = cv2.HoughLinesP(\n",
    "        dilated, rho=1, theta=np.pi/180, \n",
    "        threshold=100, minLineLength=100, maxLineGap=10\n",
    "    )\n",
    "    \n",
    "    if lines is None or len(lines) < 3:\n",
    "        print(\"Not enough lines detected, try lower thresholds\")\n",
    "        return None\n",
    "    \n",
    "    # Find horizontal-ish lines (court boundaries)\n",
    "    h_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        # Calculate angle and length\n",
    "        angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi)\n",
    "        length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "        \n",
    "        # Group based on angle (horizontal-ish lines)\n",
    "        if angle < 30 or angle > 150:\n",
    "            h_lines.append((line[0], length, angle))\n",
    "    \n",
    "    # Sort by length to get the most prominent lines\n",
    "    h_lines.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the longest horizontal line for angle calculation\n",
    "    if h_lines:\n",
    "        main_line = h_lines[0][0]\n",
    "        x1, y1, x2, y2 = main_line\n",
    "        \n",
    "        # Calculate angle of rotation needed\n",
    "        angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi\n",
    "        if angle > 90:\n",
    "            angle = angle - 180\n",
    "        \n",
    "        # Visualization for debugging\n",
    "        line_frame = frame.copy()\n",
    "        cv2.line(line_frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        cv2.putText(line_frame, f\"Angle: {angle:.2f} degrees\", (50, 50), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imwrite(\"main_alignment_line.jpg\", line_frame)\n",
    "        \n",
    "        return angle\n",
    "    else:\n",
    "        print(\"No suitable horizontal lines found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595be15e-e25e-472b-aec4-00feab8fa6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_center_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Align the court to be level and center it in the frame\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video dimensions: {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    # Read first frame for court detection\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame\")\n",
    "        return False\n",
    "    \n",
    "    # Try to detect court angle\n",
    "    print(\"Detecting court alignment...\")\n",
    "    angle = detect_court_lines_for_alignment(first_frame)\n",
    "    \n",
    "    if angle is None:\n",
    "        print(\"Could not detect court angle automatically.\")\n",
    "        # Default to no rotation\n",
    "        angle = 0\n",
    "    \n",
    "    print(f\"Detected rotation angle: {angle:.2f} degrees\")\n",
    "    \n",
    "    # Calculate transformation matrix for rotation\n",
    "    # Get rotation matrix for the angle\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions after rotation to avoid cropping\n",
    "    abs_cos = abs(rotation_matrix[0, 0])\n",
    "    abs_sin = abs(rotation_matrix[0, 1])\n",
    "    new_width = int(height * abs_sin + width * abs_cos)\n",
    "    new_height = int(height * abs_cos + width * abs_sin)\n",
    "    \n",
    "    # Adjust the rotation matrix to take into account the new dimensions\n",
    "    rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "    \n",
    "    # Apply rotation to the first frame for verification\n",
    "    rotated_first_frame = cv2.warpAffine(first_frame, rotation_matrix, (new_width, new_height))\n",
    "    cv2.imwrite(\"rotated_first_frame.jpg\", rotated_first_frame)\n",
    "    \n",
    "    # Create video writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "    \n",
    "    # Reset video capture to beginning\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    \n",
    "    # Process each frame\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "            \n",
    "            # Write rotated frame\n",
    "            out.write(rotated_frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Aligned video saved to {output_path}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4176e234-0f77-4a2c-8b6d-cd3996e2b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_rotate_and_center(input_path, output_path, angle=0):\n",
    "    \"\"\"\n",
    "    Manually rotate and center the video by a specified angle\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video dimensions: {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    # Read first frame for preview\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame\")\n",
    "        return False\n",
    "    \n",
    "    # Calculate transformation matrix for rotation\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions after rotation to avoid cropping\n",
    "    abs_cos = abs(rotation_matrix[0, 0])\n",
    "    abs_sin = abs(rotation_matrix[0, 1])\n",
    "    new_width = int(height * abs_sin + width * abs_cos)\n",
    "    new_height = int(height * abs_cos + width * abs_sin)\n",
    "    \n",
    "    # Adjust the rotation matrix to take into account the new dimensions\n",
    "    rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "    \n",
    "    # Apply rotation to the first frame for verification\n",
    "    rotated_first_frame = cv2.warpAffine(first_frame, rotation_matrix, (new_width, new_height))\n",
    "    cv2.imwrite(\"manual_rotated_preview.jpg\", rotated_first_frame)\n",
    "    \n",
    "    print(f\"Preview saved with {angle} degree rotation. Check 'manual_rotated_preview.jpg'\")\n",
    "    print(\"If you're satisfied with the rotation, proceed. Otherwise, try a different angle value.\")\n",
    "    \n",
    "    # Ask for confirmation - you can remove this part if running in a script\n",
    "    proceed = input(\"Proceed with processing the whole video? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        return False\n",
    "    \n",
    "    # Create video writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "    \n",
    "    # Reset video capture to beginning\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    \n",
    "    # Process each frame\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "            \n",
    "            # Write rotated frame\n",
    "            out.write(rotated_frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Aligned video saved to {output_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfb616d-4d0e-4dca-af7d-99712a21539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rotation_preview(input_path, angles=[-2, -1, 0, 1, 2]):\n",
    "    \"\"\"\n",
    "    Generate preview images with different rotation angles\n",
    "    \"\"\"\n",
    "    # Read the first frame\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame\")\n",
    "        return False\n",
    "    \n",
    "    height, width = frame.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    \n",
    "    for angle in angles:\n",
    "        # Get rotation matrix\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        \n",
    "        # Calculate new dimensions after rotation to avoid cropping\n",
    "        abs_cos = abs(rotation_matrix[0, 0])\n",
    "        abs_sin = abs(rotation_matrix[0, 1])\n",
    "        new_width = int(height * abs_sin + width * abs_cos)\n",
    "        new_height = int(height * abs_cos + width * abs_sin)\n",
    "        \n",
    "        # Adjust the rotation matrix to take into account the new dimensions\n",
    "        rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "        rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "        \n",
    "        # Apply rotation to the frame\n",
    "        rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "        \n",
    "        # Add text to indicate the angle\n",
    "        cv2.putText(rotated_frame, f\"Rotation: {angle} degrees\", (50, 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Save the preview image\n",
    "        preview_path = f\"rotation_preview_{angle}.jpg\"\n",
    "        cv2.imwrite(preview_path, rotated_frame)\n",
    "        print(f\"Saved preview for {angle} degrees rotation to {preview_path}\")\n",
    "    \n",
    "    print(\"Generated preview images for different rotation angles.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65552277-197a-444c-9718-7d5ab736e75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions: 1920x1080, FPS: 30.1\n",
      "Detecting court alignment...\n",
      "Detected rotation angle: -0.95 degrees\n",
      "Processing video with 300 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:06<00:00, 48.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned video saved to v3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_and_center_video(\"undist.mp4\", \"v3.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314051e2-2fa7-4ffe-bd91-5cc19eafe914",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Second attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6996e711-24bf-46d8-b456-9f8f1cbde77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_court_alignment_improved(frame):\n",
    "    \"\"\"\n",
    "    Improved method to detect court alignment angle\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "    \n",
    "    # Edge detection with lower threshold to catch more lines\n",
    "    edges = cv2.Canny(blurred, 40, 120)\n",
    "    \n",
    "    # Dilate to connect edge components\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "    \n",
    "    # Save intermediate results for debugging\n",
    "    cv2.imwrite(\"edges_detected.jpg\", edges)\n",
    "    \n",
    "    # Detect lines using Hough Lines Transform (not HoughLinesP)\n",
    "    # This gives us more precise angle measurements\n",
    "    lines = cv2.HoughLines(dilated, rho=1, theta=np.pi/180, threshold=150)\n",
    "    \n",
    "    if lines is None or len(lines) < 5:  # Need multiple lines for consensus\n",
    "        # Fall back to HoughLinesP if needed\n",
    "        print(\"Few lines detected with HoughLines, trying HoughLinesP...\")\n",
    "        lines_p = cv2.HoughLinesP(\n",
    "            dilated, rho=1, theta=np.pi/180, \n",
    "            threshold=80, minLineLength=80, maxLineGap=15\n",
    "        )\n",
    "        \n",
    "        if lines_p is None or len(lines_p) < 3:\n",
    "            print(\"Not enough lines detected with either method\")\n",
    "            return None\n",
    "            \n",
    "        # Convert HoughLinesP format to angles only\n",
    "        angles = []\n",
    "        for line in lines_p:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            # Only consider relatively horizontal lines\n",
    "            curr_angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi\n",
    "            if abs(curr_angle) < 45 or abs(curr_angle) > 135:\n",
    "                angles.append(curr_angle)\n",
    "                \n",
    "        if not angles:\n",
    "            print(\"No suitable horizontal lines found\")\n",
    "            return None\n",
    "    else:\n",
    "        # Process HoughLines output (rho, theta format)\n",
    "        angles = []\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            # Convert theta to degrees\n",
    "            angle_rad = theta\n",
    "            angle_deg = angle_rad * 180 / np.pi - 90  # -90 to get the actual line angle\n",
    "            \n",
    "            # Only consider relatively horizontal lines (near 0 or 180 degrees)\n",
    "            if abs(angle_deg) < 45 or abs(angle_deg - 180) < 45 or abs(angle_deg + 180) < 45:\n",
    "                # Normalize angle to -90 to +90 range\n",
    "                if angle_deg > 90:\n",
    "                    angle_deg -= 180\n",
    "                elif angle_deg < -90:\n",
    "                    angle_deg += 180\n",
    "                    \n",
    "                angles.append(angle_deg)\n",
    "    \n",
    "    # Create a visualization of all detected lines\n",
    "    debug_frame = frame.copy()\n",
    "    \n",
    "    if lines is not None:\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            x1 = int(x0 + 1000*(-b))\n",
    "            y1 = int(y0 + 1000*(a))\n",
    "            x2 = int(x0 - 1000*(-b))\n",
    "            y2 = int(y0 - 1000*(a))\n",
    "            cv2.line(debug_frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    \n",
    "    cv2.imwrite(\"all_detected_lines.jpg\", debug_frame)\n",
    "    \n",
    "    # Use statistical approach to find the consensus angle\n",
    "    if not angles:\n",
    "        print(\"No suitable lines found for angle calculation\")\n",
    "        return None\n",
    "    \n",
    "    # Create a histogram of angles to find the most common alignment\n",
    "    # This is more robust than just taking the average\n",
    "    hist, bin_edges = np.histogram(angles, bins=36, range=(-45, 45))\n",
    "    most_common_bin = np.argmax(hist)\n",
    "    \n",
    "    # Calculate the average angle within the most common bin\n",
    "    bin_min = bin_edges[most_common_bin]\n",
    "    bin_max = bin_edges[most_common_bin + 1]\n",
    "    filtered_angles = [a for a in angles if bin_min <= a <= bin_max]\n",
    "    \n",
    "    # Remove outliers using IQR method for more robustness\n",
    "    if filtered_angles:\n",
    "        q1 = np.percentile(filtered_angles, 25)\n",
    "        q3 = np.percentile(filtered_angles, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        final_angles = [a for a in filtered_angles if lower_bound <= a <= upper_bound]\n",
    "        \n",
    "        if not final_angles:\n",
    "            final_angles = filtered_angles  # Fallback\n",
    "            \n",
    "        final_angle = np.median(final_angles)  # Median is more robust than mean\n",
    "    else:\n",
    "        final_angle = np.median(angles)  # Fallback\n",
    "    \n",
    "    # Create visualization of the chosen angle\n",
    "    angle_vis_frame = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    length = max(w, h) // 2\n",
    "    \n",
    "    end_x = int(center_x + length * np.cos(final_angle * np.pi / 180))\n",
    "    end_y = int(center_y + length * np.sin(final_angle * np.pi / 180))\n",
    "    \n",
    "    cv2.line(angle_vis_frame, (center_x, center_y), (end_x, end_y), (0, 0, 255), 3)\n",
    "    cv2.putText(angle_vis_frame, f\"Angle: {final_angle:.2f}°\", \n",
    "              (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imwrite(\"final_alignment_angle.jpg\", angle_vis_frame)\n",
    "    \n",
    "    return final_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958d287c-0377-4d45-b011-0d423af0df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_court_video_improved(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Align the court video using improved detection method\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video dimensions: {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    # Process multiple frames for more robust detection\n",
    "    num_sample_frames = 5\n",
    "    frame_indices = np.linspace(0, min(frame_count-1, 100), num_sample_frames, dtype=int)\n",
    "    \n",
    "    angles = []\n",
    "    \n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        angle = detect_court_alignment_improved(frame)\n",
    "        if angle is not None:\n",
    "            angles.append(angle)\n",
    "            print(f\"Frame {idx}: Detected angle {angle:.2f} degrees\")\n",
    "    \n",
    "    if not angles:\n",
    "        print(\"Could not detect court angle from any sample frame\")\n",
    "        return False\n",
    "    \n",
    "    # Use median angle for robustness\n",
    "    final_angle = np.median(angles)\n",
    "    print(f\"Final rotation angle: {final_angle:.2f} degrees (median of {len(angles)} estimates)\")\n",
    "    \n",
    "    # Calculate transformation matrix for rotation\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, final_angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions after rotation to avoid cropping\n",
    "    abs_cos = abs(rotation_matrix[0, 0])\n",
    "    abs_sin = abs(rotation_matrix[0, 1])\n",
    "    new_width = int(height * abs_sin + width * abs_cos)\n",
    "    new_height = int(height * abs_cos + width * abs_sin)\n",
    "    \n",
    "    # Adjust the rotation matrix to take into account the new dimensions\n",
    "    rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "    \n",
    "    # Reset video capture to beginning and prepare output\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "    \n",
    "    # Process each frame\n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "            \n",
    "            # Write rotated frame\n",
    "            out.write(rotated_frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Aligned video saved to {output_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae39fc30-4b8c-47c3-8d05-d64f1241ec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions: 1920x1080, FPS: 30.1\n",
      "Frame 0: Detected angle 41.00 degrees\n",
      "Frame 25: Detected angle 41.00 degrees\n",
      "Frame 50: Detected angle 41.00 degrees\n",
      "Frame 75: Detected angle 41.00 degrees\n",
      "Frame 100: Detected angle 41.00 degrees\n",
      "Final rotation angle: 41.00 degrees (median of 5 estimates)\n",
      "Processing video with 300 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:10<00:00, 28.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned video saved to v5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_court_video_improved(\"undist.mp4\", \"v5.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5129f6-8e8e-4284-8e52-a3cd679b675f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Best result so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0887461-3317-4210-8eaf-926c99ac65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_court_lines_for_alignment(frame):\n",
    "    \"\"\"\n",
    "    Detect main court lines using thresholding instead of edge detection\n",
    "    \"\"\"\n",
    "    # Convert to HSV for better color segmentation\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Create mask for white lines\n",
    "    # White in HSV has high V, low S, and any H\n",
    "    lower_white = np.array([0, 0, 180])\n",
    "    upper_white = np.array([180, 70, 255])\n",
    "    white_mask = cv2.inRange(hsv, lower_white, upper_white)\n",
    "    \n",
    "    # Save the white mask for inspection\n",
    "    cv2.imwrite(\"white_mask.jpg\", white_mask)\n",
    "    \n",
    "    # Apply morphological operations to clean up the mask\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    white_mask_cleaned = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    white_mask_cleaned = cv2.morphologyEx(white_mask_cleaned, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    cv2.imwrite(\"white_mask_cleaned.jpg\", white_mask_cleaned)\n",
    "    \n",
    "    # Apply Hough Line Transform directly on the mask (preserves line width)\n",
    "    lines = cv2.HoughLinesP(\n",
    "        white_mask_cleaned, rho=1, theta=np.pi/180, \n",
    "        threshold=100, minLineLength=100, maxLineGap=20\n",
    "    )\n",
    "    \n",
    "    if lines is None or len(lines) < 3:\n",
    "        print(\"Not enough lines detected, trying with lower threshold...\")\n",
    "        # Try again with lower threshold\n",
    "        lines = cv2.HoughLinesP(\n",
    "            white_mask_cleaned, rho=1, theta=np.pi/180, \n",
    "            threshold=50, minLineLength=50, maxLineGap=30\n",
    "        )\n",
    "        \n",
    "        if lines is None or len(lines) < 3:\n",
    "            print(\"Still not enough lines detected\")\n",
    "            return None\n",
    "    \n",
    "    # Visualization of all detected lines\n",
    "    line_frame = frame.copy()\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(line_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(\"all_lines_detected.jpg\", line_frame)\n",
    "    \n",
    "    # Find horizontal-ish lines (court boundaries)\n",
    "    h_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        # Calculate angle and length\n",
    "        dx = x2 - x1\n",
    "        dy = y2 - y1\n",
    "        angle = np.abs(np.arctan2(dy, dx) * 180 / np.pi)\n",
    "        length = np.sqrt(dx*dx + dy*dy)\n",
    "        \n",
    "        # Group based on angle (horizontal-ish lines)\n",
    "        if angle < 30 or angle > 150:\n",
    "            h_lines.append((line[0], length, angle))\n",
    "    \n",
    "    # Sort by length to get the most prominent lines\n",
    "    h_lines.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the longest horizontal lines\n",
    "    longest_lines = h_lines[:min(3, len(h_lines))]\n",
    "    \n",
    "    # Calculate weighted average angle based on line length\n",
    "    if longest_lines:\n",
    "        total_weight = 0\n",
    "        weighted_angle_sum = 0\n",
    "        \n",
    "        for line_data, length, line_angle in longest_lines:\n",
    "            x1, y1, x2, y2 = line_data\n",
    "            # Calculate proper angle for rotation\n",
    "            angle = np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi\n",
    "            if angle > 90:\n",
    "                angle = angle - 180\n",
    "                \n",
    "            # Use length as weight\n",
    "            weighted_angle_sum += angle * length\n",
    "            total_weight += length\n",
    "            \n",
    "            # Visualization for debugging - show the lines used for angle calculation\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
    "        \n",
    "        # Weighted average angle\n",
    "        final_angle = weighted_angle_sum / total_weight\n",
    "        \n",
    "        # Add angle text to the visualization\n",
    "        cv2.putText(frame, f\"Angle: {final_angle:.2f} degrees\", (50, 50), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imwrite(\"main_alignment_lines.jpg\", frame)\n",
    "        \n",
    "        return final_angle\n",
    "    else:\n",
    "        print(\"No suitable horizontal lines found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866654f4-c84d-4364-941b-fcc37f4d499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_center_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Align the court to be level and center it in the frame\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video dimensions: {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    # Read first frame for court detection\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame\")\n",
    "        return False\n",
    "    \n",
    "    # Try to detect court angle\n",
    "    print(\"Detecting court alignment...\")\n",
    "    angle = detect_court_lines_for_alignment(first_frame)\n",
    "    \n",
    "    if angle is None:\n",
    "        print(\"Could not detect court angle automatically.\")\n",
    "        # Default to no rotation\n",
    "        angle = 0\n",
    "    \n",
    "    print(f\"Detected rotation angle: {angle:.2f} degrees\")\n",
    "    \n",
    "    # Calculate transformation matrix for rotation\n",
    "    # Get rotation matrix for the angle\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions after rotation to avoid cropping\n",
    "    abs_cos = abs(rotation_matrix[0, 0])\n",
    "    abs_sin = abs(rotation_matrix[0, 1])\n",
    "    new_width = int(height * abs_sin + width * abs_cos)\n",
    "    new_height = int(height * abs_cos + width * abs_sin)\n",
    "    \n",
    "    # Adjust the rotation matrix to take into account the new dimensions\n",
    "    rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "    \n",
    "    # Apply rotation to the first frame for verification\n",
    "    rotated_first_frame = cv2.warpAffine(first_frame, rotation_matrix, (new_width, new_height))\n",
    "    cv2.imwrite(\"rotated_first_frame.jpg\", rotated_first_frame)\n",
    "    \n",
    "    # Create video writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "    \n",
    "    # Reset video capture to beginning\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    \n",
    "    # Process each frame\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "            \n",
    "            # Write rotated frame\n",
    "            out.write(rotated_frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Aligned video saved to {output_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82130da5-d500-44c5-ad12-7ce03c468f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lines_from_multiple_frames(input_path, num_frames=5):\n",
    "    \"\"\"\n",
    "    Detect court lines from multiple frames for more robust angle detection\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return None\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate frame indices to sample evenly throughout the video\n",
    "    step = max(1, frame_count // (num_frames + 1))\n",
    "    frame_indices = [i * step for i in range(1, num_frames + 1)]\n",
    "    \n",
    "    angles = []\n",
    "    \n",
    "    for idx, frame_idx in enumerate(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(f\"Failed to read frame at index {frame_idx}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing frame {idx+1}/{len(frame_indices)} (frame index: {frame_idx})\")\n",
    "        \n",
    "        # Save the frame for reference\n",
    "        cv2.imwrite(f\"sample_frame_{idx+1}.jpg\", frame)\n",
    "        \n",
    "        # Detect angle\n",
    "        angle = detect_court_lines_for_alignment(frame)\n",
    "        \n",
    "        if angle is not None:\n",
    "            angles.append(angle)\n",
    "            print(f\"Frame {idx+1}: Detected angle = {angle:.2f} degrees\")\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not angles:\n",
    "        print(\"Failed to detect any valid angles\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate median angle to avoid outliers\n",
    "    median_angle = np.median(angles)\n",
    "    print(f\"Angles detected: {angles}\")\n",
    "    print(f\"Median angle: {median_angle:.2f} degrees\")\n",
    "    \n",
    "    return median_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832eb2df-6da0-4b6e-90a3-ee7c1ad3591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_rotate_and_center(input_path, output_path, angle=0):\n",
    "    \"\"\"\n",
    "    Manually rotate and center the video by a specified angle\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file {input_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video dimensions: {width}x{height}, FPS: {fps}\")\n",
    "    \n",
    "    # Read first frame for preview\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame\")\n",
    "        return False\n",
    "    \n",
    "    # Calculate transformation matrix for rotation\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    \n",
    "    # Calculate new dimensions after rotation to avoid cropping\n",
    "    abs_cos = abs(rotation_matrix[0, 0])\n",
    "    abs_sin = abs(rotation_matrix[0, 1])\n",
    "    new_width = int(height * abs_sin + width * abs_cos)\n",
    "    new_height = int(height * abs_cos + width * abs_sin)\n",
    "    \n",
    "    # Adjust the rotation matrix to take into account the new dimensions\n",
    "    rotation_matrix[0, 2] += (new_width - width) // 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) // 2\n",
    "    \n",
    "    # Apply rotation to the first frame for verification\n",
    "    rotated_first_frame = cv2.warpAffine(first_frame, rotation_matrix, (new_width, new_height))\n",
    "    cv2.imwrite(\"manual_rotated_preview.jpg\", rotated_first_frame)\n",
    "    \n",
    "    print(f\"Preview saved with {angle} degree rotation. Check 'manual_rotated_preview.jpg'\")\n",
    "    print(\"If you're satisfied with the rotation, proceed. Otherwise, try a different angle value.\")\n",
    "    \n",
    "    # Ask for confirmation - you can remove this interactive part if needed\n",
    "    proceed = input(\"Proceed with processing the whole video? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        return False\n",
    "    \n",
    "    # Create video writer for output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "    \n",
    "    # Reset video capture to beginning\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    print(f\"Processing video with {frame_count} frames...\")\n",
    "    \n",
    "    # Process each frame\n",
    "    with tqdm(total=frame_count) as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated_frame = cv2.warpAffine(frame, rotation_matrix, (new_width, new_height))\n",
    "            \n",
    "            # Write rotated frame\n",
    "            out.write(rotated_frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Aligned video saved to {output_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b33f840-3924-4ce0-bc81-410ea74a93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 1/5 (frame index: 50)\n",
      "Frame 1: Detected angle = -1.99 degrees\n",
      "Processing frame 2/5 (frame index: 100)\n",
      "Frame 2: Detected angle = -1.99 degrees\n",
      "Processing frame 3/5 (frame index: 150)\n",
      "Frame 3: Detected angle = -1.97 degrees\n",
      "Processing frame 4/5 (frame index: 200)\n",
      "Frame 4: Detected angle = -1.99 degrees\n",
      "Processing frame 5/5 (frame index: 250)\n",
      "Frame 5: Detected angle = -1.99 degrees\n",
      "Angles detected: [np.float64(-1.9932108826292696), np.float64(-1.9909618779330664), np.float64(-1.968076712069241), np.float64(-1.991809865891137), np.float64(-1.9945693012418506)]\n",
      "Median angle: -1.99 degrees\n",
      "Video dimensions: 1920x1080, FPS: 30.1\n",
      "Preview saved with -1.991809865891137 degree rotation. Check 'manual_rotated_preview.jpg'\n",
      "If you're satisfied with the rotation, proceed. Otherwise, try a different angle value.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Proceed with processing the whole video? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video with 300 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 300/300 [00:06<00:00, 48.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned video saved to v1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_video = \"undist.mp4\"  # Use the output from previous distortion correction step\n",
    "output_video = \"v1.mp4\"\n",
    "\n",
    "\n",
    "angle = detect_lines_from_multiple_frames(input_video, num_frames=5)\n",
    "\n",
    "if angle is not None:\n",
    "    # Use the detected angle for alignment\n",
    "    manual_rotate_and_center(input_video, output_video, angle=angle)\n",
    "else:\n",
    "    print(\"Falling back to automatic alignment from a single frame\")\n",
    "    align_and_center_video(input_video, output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3b43f-8baf-4ae3-afe4-67755cc2095c",
   "metadata": {},
   "source": [
    "## Player tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5069627b-cc25-4fb2-aeec-33ef1187b5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnorfair\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Detection, Tracker, Video, draw_tracked_objects\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/norfair/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdrawing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Detection, Tracker\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FilterPyKalmanFilterFactory, OptimizedKalmanFilterFactory, NoFilterFactory\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cutout, print_objects_as_table\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/norfair/tracker.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrich\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mprint\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_points\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FilterPyKalmanFilterFactory\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTracker\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     14\u001b[39m         distance_function: Callable[[\u001b[33m\"\u001b[39m\u001b[33mDetection\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTrackedObject\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mfloat\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         past_detections_length: \u001b[38;5;28mint\u001b[39m = \u001b[32m4\u001b[39m\n\u001b[32m     22\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/norfair/filter.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfilterpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkalman\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KalmanFilter\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFilterPyKalmanFilterFactory\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, R: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m4.0\u001b[39m, Q: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.1\u001b[39m, P: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m10.0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/filterpy/kalman/__init__.py:22\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"Copyright 2015 Roger R Labbe Jr.\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[33;03mFilterPy library.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03mfor more information.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (absolute_import, division, print_function,\n\u001b[32m     20\u001b[39m                         unicode_literals)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mEKF\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble_kalman_filter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfading_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/filterpy/kalman/EKF.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dot, zeros, eye\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfilterpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logpdf\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfilterpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pretty_str, reshape_z\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/linalg/__init__.py:205\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cythonized_array_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp_lu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/linalg/_basic.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs, _compute_lwork\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError, _datacopied, LinAlgWarning\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_validated\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _decomp, _decomp_svd\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_solve_toeplitz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m levinson\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/linalg/_decomp.py:24\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (array, isfinite, inexact, nonzero, iscomplexobj,\n\u001b[32m     21\u001b[39m                    flatnonzero, conj, asarray, argsort, empty,\n\u001b[32m     22\u001b[39m                    iscomplex, zeros, einsum, eye, inf)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Local imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_validated\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_misc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinAlgError, _datacopied, norm\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs, _compute_lwork\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/_lib/_util.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[32m     17\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1410\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/numpy/__init__.py:340\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__dir__\u001b[39m():\n\u001b[32m    337\u001b[39m     public_symbols = \u001b[38;5;28mglobals\u001b[39m().keys() | {\u001b[33m'\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m    338\u001b[39m     public_symbols -= {\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmatrixlib\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModuleDeprecationWarning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVisibleDeprecationWarning\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    342\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplexWarning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTooHardError\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAxisError\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m     }\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/numpy/random/__init__.py:180\u001b[39m\n\u001b[32m    126\u001b[39m __all__ = [\n\u001b[32m    127\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbinomial\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzipf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    177\u001b[39m ]\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/job/jobenv/lib/python3.12/site-packages/numpy/random/_pickle.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtrand\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomState\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_philox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pcg64\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1\u001b[39m, in \u001b[36minit numpy.random.mtrand\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from norfair import Detection, Tracker, Video, draw_tracked_objects\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c2325-1448-4d7e-8351-eb46e99afe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ CONFIGURATION ------------\n",
    "VIDEO_PATH = \"input.mp4\"\n",
    "OUTPUT_PATH = \"output_tracked.mp4\"\n",
    "YOLO_MODEL = \"yolov8n.pt\"  # use 'yolov8s.pt' for better accuracy\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "IOU_THRESHOLD = 0.4\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23348d80-4d53-411b-8b58-4899145600e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO\n",
    "model = YOLO(YOLO_MODEL)# Define distance function for Norfair\n",
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9692a-5c13-407b-8119-eaed0e723202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracker\n",
    "tracker = Tracker(distance_function=euclidean_distance, distance_threshold=30)\n",
    "\n",
    "# Open video input/output\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobenv",
   "language": "python",
   "name": "jobenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
